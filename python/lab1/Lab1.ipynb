{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LAB 1: Binary classification and model selection\n",
    "Authors: \n",
    "\n",
    "    Mathurin Massias (mathurin.massias@gmail.com)\n",
    "    \n",
    "    Giacomo Meanti  (giacomo.meanti@gmail.com)\n",
    "\n",
    "\n",
    " - This lab addresses binary classification and model selection on synthetic data.\n",
    " - The aim of the lab is to play with the libraries and to get a practical grasp of what\n",
    "we have discussed in class.\n",
    " - Follow the instructions below.\n",
    " \n",
    " \n",
    "**Goal**\n",
    "This lab is divided in three parts depending of their level of complexity (Beginner,\n",
    "Intermediate, Advanced). Your goal is to complete entirely, at least, one of the\n",
    "three parts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from scipy.io import loadmat\n",
    "\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.kernel_ridge import KernelRidge\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import datasets, model_selection\n",
    "\n",
    "from lab1_utils import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Warmup\n",
    "\n",
    "You will use a regularized least square model for classification on a toy dataset. \n",
    "You will explore the differences between training and test error, and how the regularization parameter affects them.\n",
    "\n",
    "We will use cross-validation to estimate the best values for the parameter.\n",
    "\n",
    "In many cases we commented with `# TODO` the places in which you should insert some code.\n",
    "\n",
    "### Data Generation\n",
    "\n",
    "The `create_random_data` function is used throughout this lab to generate random datasets. If you want to see how it works, look at the lab1_utils.py file in this folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = create_random_data(n_samples=100, noise_level=1.3, dataset=\"linear\")\n",
    "print(\"%d samples, %d features\" % X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_dataset(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Splitting the data into train and test\n",
    "\n",
    "We use a sklearn utils function `train_test_split` to subdivide the data into 20 testing samples, and (correspondingly) 80 training samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=20, random_state=2006)\n",
    "print(\"Generated %d training samples, %d test samples\" % (X_train.shape[0], X_test.shape[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training a linear ridge-regression model\n",
    "\n",
    "We will use the `sklearn.kernel_ridge.KernelRidge` class from scikit-learn to define our models, specifying that we want the \"linear\" kernel. Then the only parameter is the regularization parameter which we look into in this section.\n",
    "\n",
    "This estimator is meant for regression : we use it as such and take the sign of the prediciton to predict a label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def binary_classif_error(y_true, y_pred):\n",
    "    return np.mean(np.sign(y_pred) != y_true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regularization = 0.1\n",
    "\n",
    "model = KernelRidge(regularization, kernel=\"linear\")\n",
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_preds = # TODO\n",
    "test_preds = # TODO\n",
    "\n",
    "print(\"Training error: %.2f%%\" % (binary_classif_error(y_train, train_preds) * 100))\n",
    "print(\"Test error: %.2f%%\" % (binary_classif_error(y_test, test_preds) * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_separation(X_train, y_train, model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploring the effect of different parameters\n",
    "\n",
    "First we change the regularization parameter, observing what happens. Since the data is very low-dimensional, the change is not visible until reaching very large amounts of regularization.\n",
    "\n",
    "For now calculate the test errors (no cross-validation is needed)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Change the regularization parameter\n",
    "reg_values = np.geomspace(1e-4, 5e3, num=50)\n",
    "test_errors = []\n",
    "for reg in reg_values:\n",
    "    # TODO: Create the model and compute the test predictions\n",
    "    test_preds = # TODO\n",
    "    test_errors.append(binary_classif_error(y_test, test_preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.semilogx(reg_values, test_errors)\n",
    "ax.set_xlabel(\"Regularization\")\n",
    "ax.set_ylabel(\"Test error\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Change in number of data-points\n",
    "num_points = np.arange(5, 1000, 10)\n",
    "np_test_errors = []\n",
    "# TODO: Create the model\n",
    "for points in num_points:\n",
    "    X, y = create_random_data(points + 20, 1, seed=932)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=points, random_state=0)\n",
    "    # TODO: Fit/Predict\n",
    "    np_test_errors.append(binary_classif_error(y_test, test_preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.plot(num_points, np_test_errors)\n",
    "ax.set_xlabel(\"Number of training points\")\n",
    "ax.set_ylabel(\"Test error\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Amount of noise in the data\n",
    "data_noise = [0.3, 0.5, 1.0, 2.0]\n",
    "noise_test_errors = []\n",
    "# TODO: Fill the whole example in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.plot(data_noise, noise_test_errors)\n",
    "ax.set_xlabel(\"Data noise\")\n",
    "ax.set_ylabel(\"Test error\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross-Validation\n",
    "\n",
    "Find the optimal value for the regularization parameter. By using K-fold cross-validation, you can increase the confidence that good parameter settings will still be valid on the test set.\n",
    "\n",
    "Remember that you should only look at the test set at the very end, to avoid overfitting to it.\n",
    "In a real-world setting, you will not know what the test data looks like, and relying on cross-validation is one way to reduce overfitting to the training data.\n",
    "\n",
    "For this exercise it is easy to check different values of the regularization parameter by hand. For more complex scenarios, scikit-learn includes some useful classes which greatly reduce the amount of boilerplate code needed for tuning hyperparameters. For example, look at [sklearn.model_selection.GridSearchCV](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html) and [sklearn.model_selection.RandomizedSearchCV](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.RandomizedSearchCV.html)\n",
    "\n",
    "In the case of our data, we have already seen that several regularization parameters seem to work well.\n",
    "In the following it is sufficient to find one of them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg_params = [1e-7, 1e-5, 1e-3, 1e-1, 1e1, 1e3]\n",
    "kfold_cv = model_selection.KFold(n_splits=5, shuffle=True, random_state=102)\n",
    "errors = {rp: [] for rp in reg_params}\n",
    "\n",
    "# Loop through all possible regularization parameters\n",
    "for rp in reg_params:\n",
    "    model = KernelRidge(rp, kernel=\"linear\")\n",
    "    # Run K-Fold CV (on train data)\n",
    "    for train_index, val_index in kfold_cv.split(X_train):\n",
    "        # TODO: Fit the model on the training data, and calculate the error on the validation data.\n",
    "        error = 0 # TODO!\n",
    "        errors[rp].append(error)\n",
    "\n",
    "# Calculate the best error and corresponding regularization parameter\n",
    "min_rp, min_es = min(errors.items(), key=lambda kv: np.mean(kv[1]))\n",
    "print(\"The regularization parameter with minimal error is %e\" % (min_rp))\n",
    "print(\"Achieving a 5-fold CV average error of %.2f%%\" % (np.mean(min_es)*100))    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Kernel ridge regression\n",
    "\n",
    "Here we will use the same model as in Part 1, but instead of taking the linear kernel, which is equivalent to performing linear ridge regression, we take a different kernel.\n",
    "\n",
    "Different kernels can have different parameters. For example, the Gaussian (or RBF) kernel is defined by its length-scale, or `sigma`.\n",
    "\n",
    "To use the Gaussian kernel with the `KernelRidge` estimator, pass `kernel=\"rbf\"` to it instead of `kernel=\"linear\"`.\n",
    "Note that in scikit-learn, the Gaussian kernel has a `gamma` parameter which is defined as $\\gamma = \\dfrac{1}{2\\sigma^2}$. So be careful that a large $\\gamma$ corresponds to small $\\sigma$ and viceversa.\n",
    "\n",
    "### Tasks:\n",
    " 1. Perform parameter tuning for kernel ridge regression with a Gaussian kernel:\n",
    "     - Try different (`gamma`, `regularization`) pairs and compare the obtained training and test errors\n",
    "     - Fix the regularization and observe the effect of changing the length-scale `gamma`\n",
    "     - Fix `gamma` and observe the effect of changing the regularization\n",
    "     - Do you notice (and if so, when) any overfitting/oversmoothing effects?\n",
    "    Try to confirm your results by exploring a range of parameters and plotting the training and test errors.\n",
    "    \n",
    " 2. Consider the Polynomial kernel now (can be selected with `kernel=\"polynomial\"`) and perform parameter tuning over its parameters. Note that the polynomial kernel has three different parameters (`gamma`, `degree`, and `coef0`).\n",
    "     Compare the performances of the polynomial and Gaussian kernels on the *circles* and *moons* datasets with respect to the training set size (e.g. `[10, 20, 50, 100, 1000]`) and the amount of regularization (e.g. `[0.5, 0.1, 0.01, 0.001, 0.0001]`)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generate circles data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = create_random_data(n_samples=1000, noise_level=0.05, dataset=\"circles\", seed=932)\n",
    "plot_dataset(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Set-up KRR with Gaussian kernel example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# keep 20 % of data for testing\n",
    "X_train, X_test, y_train, y_test = traint_test_split(X, y, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g_model = KernelRidge(0.01, kernel=\"rbf\", gamma=0.01)\n",
    "g_model.fit(X_train, y_train)\n",
    "g_err = binary_classif_error(y_test, g_model.predict(X_test))\n",
    "print(\"Test error of Gaussian kernel with gamma=%.2f, regularization=%.2f : %.2f%%\" %\n",
    "     (g_model.gamma, g_model.alpha, g_err * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_separation(X_test, y_test, g_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Find the best parameter values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Does the Gaussian kernel overfit? If so, for which parameters?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compare Gaussian and Polynomial kernels on the circles and moons datasets\n",
    "\n",
    "Since the Polynomial kernel has many parameters, you can perform a full grid search to understand how these parameters interact. We have provided you with a skeleton code for the grid search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {\n",
    "    \"coef0\": [0, 1], # TODO: Insert some values to test\n",
    "    \"degree\": [2, 3, 4], # TODO: Insert some values to test\n",
    "    \"gamma\": [0.01, 1, 10] # TODO: Insert some values to test\n",
    "}\n",
    "model = KernelRidge(1.0, kernel=\"polynomial\")\n",
    "gs = model_selection.GridSearchCV(model, param_grid)#, scoring=\"accuracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO! Fit the Grid Search\n",
    "gs.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Have a look at the results (hint: look at the cv_results_ attribute)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate results\n",
    "print(\"Best estimator: \", gs.best_estimator_)\n",
    "gs.best_estimator_.fit(X_train, y_train)\n",
    "test_preds = gs.best_estimator_.predict(X_test)\n",
    "print(\"Test error: %.2f\" % (binary_classif_error(y_test, test_preds)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Run the same for the moons dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Compare the polynomial and gaussian kernels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Challenge\n",
    "\n",
    "The challenge consists in a learning task using a real dataset, namely **USPS (United\n",
    "States Postal Service)**: This dataset contains a number of handwritten digits images. \n",
    "The problem is to train *the best KR classifier* that is able to discriminate between the digits *1* and *7*.\n",
    "\n",
    "The data should be in the git repository ('challenge_datasets' folder, please see the `load_challenge` function).\n",
    "\n",
    "Once the classifiers are trained, they must be exported by means of the `save_challenge` function.\n",
    "\n",
    "**Submission:** You should upload your results, as generated by the `save_challenge` function  to the link:\n",
    "https://www.dropbox.com/request/K4vX1jIOGiX8hks6mtJF by the end of the challenge session.\n",
    "The results will be presented during the next class. The score of your result\n",
    "will be based on the accuracy of the classifier on a completely independently\n",
    "sampled test set.\n",
    "\n",
    "**Deadline:** 6 PM\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_challenge():\n",
    "    \"\"\"Loads the USPS one and seven digits.\n",
    "    \"\"\"\n",
    "    one = loadmat(\"../../data/one_train.mat\")[\"one_train\"]\n",
    "    seven = loadmat(\"../../data/seven_train.mat\")[\"seven_train\"]\n",
    "    X = np.concatenate((one, seven), 0)\n",
    "    Y = np.ones((X.shape[0], ))\n",
    "    Y[one.shape[0]: ] = -1\n",
    "    return X, Y\n",
    "\n",
    "def save_challenge(name, regularization, kernel, kernel_parameters, train_err):\n",
    "    \"\"\"Save your solution to a file.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    name\n",
    "        A string in `name-surname` format (e.g. \"giacomo-meanti\")\n",
    "    regularization\n",
    "        The amount of regularization used\n",
    "    kernel\n",
    "        A string representing the kernel you used (this can be \"linear\", \"gaussian\", \"polynomial\")\n",
    "    kernel_parameters\n",
    "        A tuple containing the parameters of your kernel. For linear kernels this will be the empty\n",
    "        tuple `()`; for the gaussian kernel it will contain one parameter for gamma, and for the\n",
    "        polynomial kernel it should contain three parameters: gamma, degree and coef0\n",
    "    train_err\n",
    "        The binary classification error obtained on the data provided.\n",
    "    \"\"\"\n",
    "    if not isinstance(name, str):\n",
    "        raise ValueError(\"Name (%s) must be a string\" % (name))\n",
    "    try:\n",
    "        regularization = float(regularization)\n",
    "    except TypeError:\n",
    "        raise ValueError(\"regularization (%s) must be a numeric value\" % (regularization))\n",
    "    if not isinstance(kernel, str):\n",
    "        raise ValueError(\"kernel (%s) must be a string\" % (kernel))\n",
    "    try:\n",
    "        train_err = float(train_err)\n",
    "    except TypeError:\n",
    "        raise ValueError(\"train_err (%s) must be a numeric value\" % (train_err))\n",
    "    \n",
    "    \n",
    "    save_struct = {\n",
    "        \"name\": name,\n",
    "        \"regularization\": regularization,\n",
    "        \"kernel\": kernel,\n",
    "        \"kernel_parameters\": kernel_parameters,\n",
    "        \"train_err\": train_err\n",
    "    }\n",
    "    out_file_name = \"%s.pkl\" % (name)\n",
    "    with open(out_file_name, \"wb\") as out_file:\n",
    "        pickle.dump(save_struct, out_file)\n",
    "    print(\"%s you learned with success!\\n\\nyour learned model uses %s kernel, \"\n",
    "          \"kernel parameters %s and regularization parameter %f. \\n\\n\"\n",
    "          \"Its error is %2.2f%% \\n\\n\"\n",
    "          \"AND REMEMBER: if you like it submit your solution!!\" % \n",
    "          (name, kernel, kernel_parameters, regularization, train_err*100))\n",
    "    print(\"\\nThe solution file was saved to '%s'\" % (out_file_name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We provide an example below of how to load the data, run a simple linear classifier, and save the results to a file. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = load_challenge()\n",
    "X.shape, y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regularization = 200\n",
    "\n",
    "k = KernelRidge(regularization, kernel=\"linear\")\n",
    "k.fit(X, y)\n",
    "err = binary_classif_error(y, k.predict(X))\n",
    "print(\"error: \", err)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name = \"\" # TODO: INSERT YOUR NAME-SURNAME (e.g. \"giacomo-meanti\")\n",
    "save_challenge(name,\n",
    "               regularization=regularization, kernel=\"linear\",\n",
    "               kernel_parameters=(), train_err=err)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Write your own model!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
