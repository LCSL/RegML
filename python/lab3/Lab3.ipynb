{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LAB3: Sparsity\n",
    "Authors: \n",
    "\n",
    "    Mathurin Massias (mathurin.massias@gmail.com)\n",
    "    \n",
    "    Giacomo Meanti  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "\n",
    "from scipy.io import loadmat\n",
    "\n",
    "from sklearn.linear_model import ElasticNet, ElasticNetCV\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from lab3_utils import create_random_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Warm-up part"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset generation and model fitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# what does create_random_data do ?\n",
    "print(create_random_data.__doc__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test_data(n_samples, n_features, n_informative_features, \n",
    "                    noise_level):\n",
    "    \"\"\"Util function to generate and split random data.\n",
    "    See the docstring of create_random_data for more details.\n",
    "    \"\"\"\n",
    "    X, y = create_random_data(n_samples, n_features, n_informative_features, \n",
    "                              noise_level=noise_level)\n",
    "    print(\"X shape:\", X.shape)\n",
    "    print(\"y shape:\", y.shape)\n",
    "    train_size = 0.8  # proportion of dataset used for training\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, shuffle=False, train_size=train_size)\n",
    "    return X_train, X_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Noiseless data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, generate data without noise: $y = X \\beta^\\star$, where $\\beta^\\star$ has `n_informative_features` non-zero entries and $X$ has shape (n_samples, n_features).\n",
    "\n",
    "We split it into testing and training parts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_samples = 100\n",
    "n_features = 200\n",
    "n_informative_features = 50\n",
    "\n",
    "# first, use noiseless data, split in train and test/validation parts\n",
    "X_train, X_test, y_train, y_test = train_test_data(\n",
    "    n_samples, n_features, n_informative_features, noise_level=0.)\n",
    "print(\"Training dataset shape:\", X_train.shape)\n",
    "print(\"Testing dataset shape:\", X_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In sklearn, the objective function of the `ElasticNet` optimization is:\n",
    "$$\\frac{1}{2 \\times \\text{n_samples}} \\Vert y - X \\beta \\Vert_2^2 + \\alpha \\times \\left( \\text{l1_ratio} \\times \\Vert \\beta \\Vert_1 + \\frac{1 - \\text{l1_ratio}}{2} \\Vert \\beta \\Vert_2^2\\right)$$\n",
    "you can get pure L1 regularization (the LASSO estimator) by setting `l1_ratio` to $1$, and pure L2 regularization (ridge regression) by setting it to $0$.\n",
    "\n",
    "The datafit normalisation by `n_samples`, frequently used in statistics, is here to make `alpha` insensitive to the number of samples, in a CV setting for example.\n",
    "\n",
    "For more information, read the docstring after displaying it in the next cell (you can close the documentation popup afterwards by clicking on the cross or hitting Esc)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ElasticNet?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# instantiate a classifier with arbitrary values for L1 and L2 penalization\n",
    "clf = ElasticNet(alpha=0.01, l1_ratio=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# fit the model and print its first coefficients\n",
    "# beware that sklearn fits an intercept by default\n",
    "clf.fit(X_train, y_train)\n",
    "print(\"50 first coefficients of estimated w:\\n\", clf.coef_[:50])\n",
    "print(\"Intercept: %f\" % clf.intercept_)\n",
    "print(\"Nonzero coefficients: %d out of %d\" % ((clf.coef_ != 0.).sum(), clf.coef_.shape[0]))\n",
    "print(\"Training error: %.2e\" % np.mean((y_train - clf.predict(X_train)) ** 2))\n",
    "# TODO compute testing error on left out data\n",
    "# print(\"Testing error: %.4f\" % )\n",
    "# TODO bonus: why is clf.predict(X_train) not equal to X_train @ clf.coef_? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For a fixed $\\alpha$, test the influence of l1_ratio on the sparsity of the solution and on the behaviors of the train and test errors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l1_ratios = [0., 0., 0., 0., 0.]  # TODO choose your own values between 0 and 1\n",
    "\n",
    "train_errs = np.zeros(len(l1_ratios))\n",
    "test_errs = np.zeros_like(train_errs)\n",
    "sparsity = np.zeros_like(train_errs)\n",
    "\n",
    "for i, l1_ratio in enumerate(l1_ratios):\n",
    "    clf = # TODO; you may need to tune alpha a bit too.\n",
    "    # TODO fit the model on train data\n",
    "    # TODO compute train and test errors\n",
    "    train_errs[i] = \n",
    "    test_errs[i] = \n",
    "    sparsity[i] = # number of non-zero elements in clf.coef_\n",
    "    \n",
    "plt.figure()\n",
    "plt.plot(l1_ratios, test_errs, label='Test error')\n",
    "plt.plot(l1_ratios, train_errs, label='Train error')\n",
    "plt.xlabel(\"l1_ratio\")\n",
    "plt.legend()\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(l1_ratios, sparsity)\n",
    "plt.ylabel(r'$||w||_0$')\n",
    "plt.xlabel('l1_ratio')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO do the same for the influence of alpha, with a fixed l1_ratio\n",
    "# What happens when alpha becomes too big?\n",
    "alphas = np.geomspace(1e-4, 1e4, num=50)\n",
    "print(alphas)\n",
    "# TODO plot train/test curve, sparsity curve"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Noisy data\n",
    "Do the same analysis as above, this time when the observations $y$ are corrupted by additive Gaussian noise:\n",
    "$$y = X \\beta^\\star + \\varepsilon$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO check again the influence of regularization when there is noise in the data.\n",
    "noise_level = 0.5\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_data(\n",
    "    n_samples, n_features, n_informative_features, \n",
    "    noise_level=noise_level)\n",
    "# TODO: plot train/test curve and sparsity curve"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bonus \n",
    "An alternative formulation/parametrization of the ElasticNet objective is:\n",
    "$$\\frac{1}{2 \\times \\text{n_samples}} \\Vert y - X \\beta \\Vert_2^2 + a \\Vert \\beta \\Vert_1 + \\frac{b}{2} \\Vert \\beta \\Vert_2^2$$\n",
    "\n",
    "Express $\\alpha$ and l1_ratio as functions of $a$ and $b$.\n",
    "\n",
    "For a fixed value of $a$, fit the model with increasing values of $b$. How is the sparsity of the solutions affected?  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO how is the sparsity affected if you increase the L2 regularization ?\n",
    "a = \n",
    "b_values = []\n",
    "sparsity = np.zeros(len(b_values))\n",
    "for i, b in enumerate(b_values):\n",
    "    alpha = \n",
    "    l1_ratio = \n",
    "    clf = \n",
    "    clf.fit(X_train, y_train)\n",
    "    sparsity[i] = \n",
    "# TODO plot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Influence of dataset size\n",
    "Observe that the datafitting term is normalized by n_samples, hence it should not grow when the dataset becomes taller (`n_features` is fixed, `n_samples` increases).\n",
    "\n",
    "Vary one of `n_samples`, `n_features` and `n_informative_features` to observe their influence on the model. What happens when `n_samples` becomes greater that `n_informative_features` ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_samples = # TODO\n",
    "n_features = # TODO\n",
    "n_informative_features = # TODO\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_data(\n",
    "    n_samples, n_features, n_informative_features, noise_level=0.5)\n",
    "\n",
    "# TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameter selection with cross validation\n",
    "In the next section, we use scikit-learn's built in functions to perform cross validated selection of alpha and l1_ratio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_data(\n",
    "    n_samples=100, n_features=300, n_informative_features=20, noise_level=1)\n",
    "# using 3-fold cross validation\n",
    "clf = ElasticNetCV(l1_ratio=[.1, .4, .8, .99,], cv=3)\n",
    "clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# displaying mean squared errors for various values of l1_ratio\n",
    "# (values are over CV folds, thick black line is average over folds)\n",
    "fig, axarr = plt.subplots(2, 2, figsize=(15, 10), \n",
    "                          constrained_layout=True, sharey=True)\n",
    "for i, l1_ratio in enumerate(clf.l1_ratio):\n",
    "    mse = clf.mse_path_[i]\n",
    "    alphas = clf.alphas_[i]\n",
    "    axarr.flat[i].semilogx(alphas, mse, ':')\n",
    "    axarr.flat[i].semilogx(alphas, mse.mean(axis=-1), 'k',\n",
    "                 label='Average across the folds', linewidth=2)\n",
    "\n",
    "    axarr.flat[i].set_xlabel(r'$\\alpha$')\n",
    "    axarr.flat[i].set_ylabel('Mean square error')\n",
    "    axarr.flat[i].set_title('l1_ratio=%s' % l1_ratio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Optimal values for l1_ratio and alpha: %s, %.2e\" % (clf.l1_ratio_, clf.alpha_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How does the best l1_ratio evolve when n_informative_features increases ? Why ? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Intermediate part"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load some data verifying $y = \\text{sign}(X \\beta^\\star)$ where $\\beta^\\star$ is unknown and $s$-sparse - but you do not know $s$. **The goal of this part is to infer $s$**.\n",
    "\n",
    "Note that this problem is notoriously difficult, even in this noiseless case, and cannot be solved without stringent assumptions on the design matrix $X$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = loadmat(\"../../data/part3-data.mat\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data[\"X\"]\n",
    "y = data[\"Y\"][:, 0]\n",
    "print(X.shape, y.shape)\n",
    "# TODO check numerically that y only contains values equal to 1 or -1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you must infer $s$.\n",
    "A first possible approach is to use the Cross-Validation procedure used in the previous part: find the sparsity of the optimal $\\beta$ obtained by cross-validation on a grid of values for $\\alpha$ and l1_ratio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO find optimal s from a CV point of view"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another way to try to estimate $s$ is to measure the correlation between\n",
    "the columns of $X$ and $y$. Indeed, the zero coefficients in $\\beta^\\star$ will ignore the\n",
    "corresponding columns in $X$ while generating $y$. Can you also identify the indices of these features ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO compute correlation\n",
    "# corr = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sort:\n",
    "idx = np.argsort(corr)\n",
    "plt.plot(corr[idx[::-1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO identify the cutoff numerically, get s \n",
    "# and the indices of highest correlated features\n",
    "# highly_corr_feats ="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, use again the code of the first part, and tune the sparsity parameter l1_ratio so that\n",
    "it selects only $s$ features ($s$ being your sparsity estimate from the previous\n",
    "question). Look at which are the selected features in your solution. Do they\n",
    "correspond to the ones you identified with the correlation approach? \n",
    "If they do not, can you figure out why does this happen?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
